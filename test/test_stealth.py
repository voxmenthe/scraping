#!/usr/bin/env python3
"""
Test script specifically for testing stealth mode and bot protection bypass.
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from scraping import DynamicWebScraper


def test_stealth_mode():
    """Test stealth mode against various protection systems."""
    
    # Test URLs that commonly have bot protection
    test_sites = [
        {
            'url': 'https://www.hims.com',
            'description': 'HIMS (likely has Cloudflare protection)',
            'expected_protection': 'cloudflare'
        },
        {
            'url': 'https://httpbin.org/user-agent',
            'description': 'HTTPBin User Agent (tests user agent spoofing)',
            'expected_protection': None
        },
        {
            'url': 'https://whatismyipaddress.com/',
            'description': 'IP detection site (tests headers)',
            'expected_protection': None
        }
    ]
    
    print("ðŸ§ª STEALTH MODE TESTING")
    print("=" * 50)
    
    for test_case in test_sites:
        url = test_case['url']
        description = test_case['description']
        
        print(f"\nðŸŽ¯ Testing: {description}")
        print(f"URL: {url}")
        print("-" * 30)
        
        # Test without stealth mode first
        print("1ï¸âƒ£ Testing WITHOUT stealth mode...")
        scraper_normal = DynamicWebScraper(
            headless=True, 
            stealth_mode=False,
            timeout=20000
        )
        
        try:
            results_normal = scraper_normal.comprehensive_scrape(url, max_retries=1)
            
            if 'error' in results_normal:
                print(f"âŒ Normal mode failed: {results_normal['error']}")
                normal_success = False
            else:
                print("âœ… Normal mode succeeded")
                protection = results_normal.get('final_structure', {}).get('protection_detected')
                if protection:
                    print(f"âš ï¸ Protection detected: {protection}")
                normal_success = True
                
        except Exception as e:
            print(f"âŒ Normal mode error: {e}")
            normal_success = False
        
        # Test with stealth mode
        print("\n2ï¸âƒ£ Testing WITH stealth mode...")
        scraper_stealth = DynamicWebScraper(
            headless=True, 
            stealth_mode=True,
            timeout=30000
        )
        
        try:
            results_stealth = scraper_stealth.comprehensive_scrape(url, max_retries=3)
            
            if 'error' in results_stealth:
                print(f"âŒ Stealth mode failed: {results_stealth['error']}")
                stealth_success = False
            else:
                print("âœ… Stealth mode succeeded")
                protection = results_stealth.get('final_structure', {}).get('protection_detected')
                if protection:
                    print(f"âš ï¸ Protection detected but bypassed: {protection}")
                else:
                    print("ðŸŽ‰ No protection detected")
                
                # Show some stats
                raw_info = results_stealth.get('final_structure', {}).get('raw_page_info', {})
                if raw_info and 'error' not in raw_info:
                    print(f"ðŸ“Š Page stats: {raw_info.get('element_count', 0)} elements, "
                          f"{raw_info.get('body_text_length', 0)} chars")
                
                attempts = results_stealth.get('attempts_made', 1)
                print(f"ðŸ”„ Completed in {attempts} attempt(s)")
                
                stealth_success = True
                
        except Exception as e:
            print(f"âŒ Stealth mode error: {e}")
            stealth_success = False
        
        # Summary
        print(f"\nðŸ“‹ Summary for {description}:")
        print(f"   Normal mode: {'âœ… Success' if normal_success else 'âŒ Failed'}")
        print(f"   Stealth mode: {'âœ… Success' if stealth_success else 'âŒ Failed'}")
        
        if stealth_success and not normal_success:
            print("   ðŸŽ¯ Stealth mode provided improvement!")
        elif stealth_success and normal_success:
            print("   âœ¨ Both modes worked")
        elif not stealth_success and not normal_success:
            print("   âš ï¸ Site may have strong protection")
        
        print("\n" + "="*50)


def test_single_site_stealth(url: str):
    """Test a single site with detailed stealth analysis."""
    
    print(f"ðŸ” DETAILED STEALTH ANALYSIS")
    print(f"Target: {url}")
    print("=" * 50)
    
    scraper = DynamicWebScraper(
        headless=False,  # Show browser for debugging
        stealth_mode=True,
        timeout=30000
    )
    
    try:
        print("ðŸš€ Starting stealth analysis...")
        results = scraper.comprehensive_scrape(url, max_retries=3)
        
        if 'error' in results:
            print(f"âŒ Analysis failed: {results['error']}")
            return False
        
        # Save detailed results
        output_file = f"stealth_analysis_{url.replace('https://', '').replace('/', '_')}.txt"
        scraper.spec_generator.save_spec_as_text(
            results['scraping_specification'], 
            output_file, 
            structure=results.get('final_structure')
        )
        
        print(f"âœ… Analysis complete! Saved to: {output_file}")
        
        # Show key information
        protection = results.get('final_structure', {}).get('protection_detected')
        if protection:
            print(f"âš ï¸ Protection detected: {protection}")
        
        attempts = results.get('attempts_made', 1)
        print(f"ðŸ”„ Completed in {attempts} attempt(s)")
        
        raw_info = results.get('final_structure', {}).get('raw_page_info', {})
        if raw_info and 'error' not in raw_info:
            print(f"ðŸ“Š Final stats:")
            print(f"   Elements: {raw_info.get('element_count', 0)}")
            print(f"   Text length: {raw_info.get('body_text_length', 0)} chars")
            print(f"   Links: {raw_info.get('link_count', 0)}")
            print(f"   Forms: {raw_info.get('form_count', 0)}")
            print(f"   Scripts: {raw_info.get('script_count', 0)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False


if __name__ == "__main__":
    if len(sys.argv) > 1:
        # Test specific URL
        url = sys.argv[1]
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        test_single_site_stealth(url)
    else:
        # Run comprehensive stealth tests
        test_stealth_mode() 