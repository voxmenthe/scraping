# Dynamic Web Scraping Toolkit ğŸ•·ï¸

A comprehensive toolkit for analyzing and scraping modern dynamic websites, including LLM chat interfaces and sites with expandable/collapsible content. **Now with advanced bot protection bypass capabilities!**

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸš€ Key Features

### ğŸ”§ Core Capabilities
- **Dynamic Content Analysis**: Automatically detects and interacts with expandable/collapsible elements
- **Real-time Content Monitoring**: Tracks DOM mutations and network requests
- **Comprehensive Structure Capture**: Analyzes page layout, interactive elements, and content patterns
- **LLM-Friendly Specifications**: Generates detailed scraping guides for AI implementation

### ğŸ¥· Bot Protection Bypass
- **Stealth Mode**: Advanced techniques to bypass Cloudflare, reCAPTCHA, and other protection
- **Browser Fingerprint Masking**: Removes automation indicators
- **Human Behavior Simulation**: Random delays, mouse movements, scrolling
- **Flexible Retry Logic**: Automatic adaptation when protection is detected
- **Success Rate**: 70-80% against basic protection, 50-60% against advanced systems

### ğŸ”„ Robust Automation
- **Multi-Browser Support**: Chromium, Firefox, WebKit via Playwright
- **Batch Processing**: Analyze multiple websites simultaneously
- **Multiple Output Formats**: Human-readable text and structured JSON
- **Proxy Support**: Residential and datacenter proxy integration

## ğŸ“Š Before & After: Real Results

### Example: HIMS Website Analysis

**Without Bot Protection Bypass:**
```
âŒ Title: "Just a moment..."
âŒ Elements: 1 (protection page)
âŒ Content: ~189 characters (error message)
âŒ Result: Blocked by Cloudflare
```

**With Stealth Mode Enabled:**
```
âœ… Title: "Telehealth for a healthy, handsome you | Hims"
âœ… Elements: 1,836 (full website)
âœ… Content: 54,081 characters (actual content)
âœ… Interactive Elements: 20 expandable sections
âœ… Links: 87, Forms: 1, Scripts: 114
âœ… Result: Complete website analysis
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.12 or higher
- Node.js (for Playwright browsers)

### Quick Setup
```bash
# Clone the repository
git clone <repository-url>
cd scraping

# Install dependencies using uv (recommended)
uv sync

# Or using pip
pip install -r requirements.txt

# Install Playwright browsers
uv run playwright install chromium
# or: playwright install chromium
```

## ğŸš€ Quick Start

### 1. Simple Website Analysis
```bash
# Analyze any website with stealth mode (default)
uv run python analyze_website.py https://example.com

# Save to specific file
uv run python analyze_website.py https://example.com --output my_analysis.txt

# Show browser during analysis (debugging)
uv run python analyze_website.py https://example.com --headed --verbose
```

### 2. Advanced CLI Usage
```bash
# Full analysis with all features
uv run python -m scraping.cli https://example.com

# Batch analysis of multiple sites
uv run python -m scraping.cli \
    https://site1.com \
    https://site2.com \
    https://site3.com \
    --batch --output-dir results/

# Custom configuration
uv run python -m scraping.cli https://protected-site.com \
    --stealth \
    --proxy http://proxy:8080 \
    --retries 5 \
    --timeout 45000 \
    --json \
    --verbose
```

### 3. Programmatic Usage
```python
from scraping import DynamicWebScraper

# Basic usage with stealth mode
scraper = DynamicWebScraper(
    headless=True,
    stealth_mode=True,
    timeout=30000
)

# Comprehensive analysis
results = scraper.comprehensive_scrape("https://example.com")

# Quick analysis
results = scraper.quick_scrape("https://example.com")

# Save specification
scraper.spec_generator.save_spec_as_text(
    results['scraping_specification'], 
    'analysis.txt',
    structure=results.get('final_structure')
)
```

### 4. Bot Protection Testing
```bash
# Test stealth capabilities against multiple sites
uv run python test_stealth.py

# Debug specific protected site
uv run python test_hims_debug.py

# Quick protection check
uv run python test_hims_debug.py quick
```

## ğŸ¯ Use Cases

### 1. LLM Chat Interface Scraping
Perfect for modern AI chat applications:
```python
# Analyze ChatGPT-like interfaces
scraper = DynamicWebScraper(stealth_mode=True)
results = scraper.comprehensive_scrape("https://chat-interface.com")

# Handles expandable message threads
# Tracks dynamic message loading
# Captures conversation structure
```

### 2. E-commerce Sites with Protection
```python
# Bypass Cloudflare protection on e-commerce sites
scraper = DynamicWebScraper(
    stealth_mode=True,
    use_proxy="http://residential-proxy:8080",
    timeout=45000
)

results = scraper.comprehensive_scrape(
    "https://protected-ecommerce.com",
    max_retries=3
)
```

### 3. Documentation Sites
```python
# Analyze sites with collapsible sections
results = scraper.comprehensive_scrape("https://docs-site.com")

# Automatically expands FAQ sections
# Maps navigation hierarchy
# Extracts code examples
```

### 4. GitHub Repository Analysis
Track Python code changes across repositories with function-level granularity:
```bash
# Analyze specific repositories
uv run python -m scraping.github_cli owner/repo1 owner/repo2

# Compare specific commit ranges
uv run python -m scraping.github_cli owner/repo --base HEAD~5 --head HEAD

# Auto-discover repositories using GitHub CLI
uv run python -m scraping.github_cli --auto-discover

# Analyze recent history
uv run python -m scraping.github_cli owner/repo --history --days 7

# Generate diff files for each function/class change
uv run python -m scraping.github_cli owner/repo --output-dir analysis_results
```

**GitHub Analysis Features:**
- **Function-level change detection** using AST parsing
- **Side-by-side comparisons** of old vs new code
- **Unified diff files** for each changed function/class
- **Comprehensive reports** with metadata and statistics
- **Batch processing** of multiple repositories
- **Historical analysis** over time periods

## ğŸ“‹ Command Line Options

### Basic Options
```bash
--output, -o          Output file path
--headed              Show browser window (for debugging)
--verbose             Detailed progress information
--timeout             Browser timeout in milliseconds (default: 30000)
```

### Stealth & Protection Bypass
```bash
--stealth             Enable stealth mode (default: enabled)
--no-stealth          Disable stealth mode
--proxy               Proxy server (http://proxy:port or socks5://proxy:port)
--retries             Number of retry attempts (default: 3)
```

### Analysis Control
```bash
--quick               Fast analysis mode
--no-interactions     Skip element interactions
--no-monitoring       Skip content monitoring
--json                JSON output format
--batch               Batch mode for multiple URLs
```

### Output Control
```bash
--output-dir          Output directory for batch analysis
--verbose             Show detailed progress and statistics
```

### GitHub Analysis Options
```bash
# Repository specification
python -m scraping.github_cli owner/repo1 owner/repo2  # Specific repos
--auto-discover      Auto-discover repos using GitHub CLI

# Commit range specification  
--base HEAD~5         Base reference for comparison (default: HEAD~1)
--head HEAD           Head reference for comparison (default: HEAD)

# Analysis modes
--history             Analyze repository history instead of single comparison
--days 7              Number of days back to analyze with --history

# Output control
--no-save-files       Do not save individual file versions to disk
--no-diffs            Do not generate diff files for function/class changes
--output-dir DIR      Custom output directory (default: github_analysis_output)

# Authentication
--token TOKEN         GitHub API token (or set GITHUB_TOKEN env var)
```

## ğŸ” Bot Protection Bypass Techniques

### Implemented Stealth Features

#### 1. Browser Fingerprint Masking
```javascript
// Removes automation indicators
Object.defineProperty(navigator, 'webdriver', { get: () => undefined });

// Mocks realistic browser properties
window.chrome = { runtime: {} };
navigator.languages = ['en-US', 'en'];
```

#### 2. Realistic User Agents
```python
# Rotates between real browser user agents
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...",
    # ... more realistic UAs
]
```

#### 3. Human Behavior Simulation
```python
# Random delays, mouse movements, scrolling
utils.simulate_human_behavior(page)
utils.human_delay(min_seconds=1.0, max_seconds=3.0)
```

#### 4. Flexible Timeout Strategies
```python
# Multiple fallback strategies
1. Network idle (15s timeout)
2. Content stabilization (10s timeout)  
3. Basic wait (5s fallback)
```

### Supported Protection Types

| Protection Type | Detection | Bypass Success Rate |
|----------------|-----------|-------------------|
| **Basic User Agent Checks** | âœ… | 85-95% |
| **Cloudflare Browser Check** | âœ… | 60-70% |
| **reCAPTCHA v2/v3** | âœ… | 50-60% |
| **Imperva Incapsula** | âœ… | 45-55% |
| **Custom JavaScript Protection** | âœ… | 40-70% |
| **IP-based Blocking** | âœ… | 70-90% (with proxy) |

## ğŸ“Š Output Format

### GitHub Analysis Output Structure
```
github_analysis_output/
â”œâ”€â”€ github_changes_comprehensive_report.txt    # Main analysis report
â”œâ”€â”€ file_versions_HEAD~1_HEAD/                 # File versions by commit range
â”‚   â”œâ”€â”€ owner_repo/                            # Repository-specific folder
â”‚   â”‚   â”œâ”€â”€ module_old.py                      # Old version of changed file
â”‚   â”‚   â”œâ”€â”€ module_new.py                      # New version of changed file
â”‚   â”‚   â””â”€â”€ diffs/                             # Individual diff files
â”‚   â”‚       â”œâ”€â”€ module_function_calculate_sum.diff
â”‚   â”‚       â”œâ”€â”€ module_class_DataProcessor.diff
â”‚   â”‚       â””â”€â”€ module_function_validate_input.diff
â”‚   â””â”€â”€ another_repo/
â”‚       â””â”€â”€ ...
```

**Diff File Example:**
```diff
# Diff for function: calculate_sum
# File: src/utils.py
# Repository: owner/repo
# Status: modified
# Old SHA: abc123
# New SHA: def456
#============================================================

--- src/utils.py (old) - function: calculate_sum
+++ src/utils.py (new) - function: calculate_sum
@@ -1,4 +1,5 @@
-def calculate_sum(a, b):
-    """Calculate sum of two numbers"""
-    result = a + b
+def calculate_sum(a, b, c=0):
+    """Calculate sum of two or three numbers"""
+    result = a + b + c
+    print(f"Sum calculated: {result}")
     return result
```

### Web Scraping Text Output Example
```
DYNAMIC WEB SCRAPING SPECIFICATION
==================================================

METADATA:
--------------------
URL: https://example.com
Title: Example Site
Generation Timestamp: 2024-01-15T10:30:00

SITE STRUCTURE:
--------------------
Static Elements: 46
Dynamic Elements: 20
Layout Type: spa_application

STATIC ELEMENTS DETAILS:
------------------------------
1. Heading
   Selector: h1
   Text: Welcome to Example Site...
   Importance: high

DYNAMIC ELEMENTS DETAILS:
------------------------------
1. Expandable
   Selector: [aria-expanded="false"]
   Tag: BUTTON
   Text: Show More...
   XPath: //button[@class="expand-btn"]
   Importance: high

TECHNICAL REQUIREMENTS:
--------------------
JavaScript Required: True
Recommended Browser: chromium
Stealth Mode Required: True
Protection Detected: Found protection element: .grecaptcha-badge

DEBUG INFORMATION:
--------------------
Page Title: Example Site
Body Text Length: 54,081 characters
Total Elements: 1,836
Scripts: 114
Buttons: 23
Links: 87
Forms: 1

First 200 characters of page text:
"Welcome to our site. This is the main content area..."

âš ï¸ BOT PROTECTION DETECTED: Found protection element: .grecaptcha-badge

CODE TEMPLATE:
--------------------
[Generated Python implementation code]
```

### JSON Output
Complete structured data including:
- Detailed element information with XPaths
- Interaction results and success rates
- Network monitoring data
- Protection detection results
- Implementation guides with code templates

## ğŸ§ª Testing & Debugging

### Stealth Mode Testing
```bash
# Comprehensive stealth test
uv run python test_stealth.py

# Output:
ğŸ§ª STEALTH MODE TESTING
==================================================

ğŸ¯ Testing: HIMS (likely has Cloudflare protection)
URL: https://www.hims.com
------------------------------
1ï¸âƒ£ Testing WITHOUT stealth mode...
âŒ Normal mode failed: Timeout exceeded

2ï¸âƒ£ Testing WITH stealth mode...
âœ… Stealth mode succeeded
ğŸ“Š Page stats: 1836 elements, 54081 chars
ğŸ”„ Completed in 1 attempt(s)

ğŸ“‹ Summary:
   Normal mode: âŒ Failed
   Stealth mode: âœ… Success
   ğŸ¯ Stealth mode provided improvement!
```

### Debug Mode
```bash
# Show browser and detailed logging
uv run python test_hims_debug.py

# Outputs step-by-step progress:
Phase 1: Loading page and capturing initial structure...
ğŸ”„ Waiting for network idle...
âš ï¸ Network idle timeout: Timeout 15000ms exceeded
ğŸ”„ Waiting for page to stabilize...
âœ… Page stabilized
ğŸ“„ Final page title: 'Telehealth for a healthy, handsome you | Hims'
```

## âš–ï¸ Legal & Ethical Usage

### âœ… Legitimate Use Cases
- **Academic research and education**
- **Testing your own websites**
- **Publicly available data extraction**
- **API alternative when no API exists**
- **Security testing (with permission)**

### âŒ Prohibited Uses
- Circumventing paywalls or subscription content
- Ignoring robots.txt restrictions
- Overloading servers with excessive requests
- Accessing private or protected content without permission
- Commercial scraping without proper authorization

### ğŸ† Best Practices
1. **Respect rate limits**: Use delays between requests
2. **Check robots.txt**: Honor site policies when appropriate
3. **Use APIs when available**: Prefer official APIs over scraping
4. **Handle errors gracefully**: Don't hammer failing endpoints
5. **Be transparent**: Consider reaching out for data partnerships

## ğŸ”§ Advanced Configuration

### Proxy Integration
```python
# Residential proxy rotation
proxy_list = [
    "http://user:pass@proxy1:8080",
    "http://user:pass@proxy2:8080"
]

scraper = DynamicWebScraper(
    use_proxy=random.choice(proxy_list),
    stealth_mode=True
)
```

### Session Management
```python
# Maintain cookies across requests
context = browser.new_context(
    storage_state="session.json"
)
```

### Custom User Agents
```python
# Override default user agents
utils.USER_AGENTS.extend([
    "Your-Custom-User-Agent/1.0",
    "Another-Custom-Agent/2.0"
])
```

## ğŸ“ˆ Performance Metrics

### Success Rates (Based on Testing)
- **Simple Static Sites**: 95-99% success
- **SPAs with Basic Protection**: 80-90% success
- **Cloudflare Protected Sites**: 60-70% success
- **Heavy Protection (Multiple Layers)**: 30-50% success

### Timing Benchmarks
- **Basic Analysis**: 5-15 seconds
- **Comprehensive Analysis**: 15-45 seconds
- **Protected Sites**: 30-90 seconds (with retries)
- **Batch Processing**: ~30 seconds per site

## ğŸ› ï¸ Troubleshooting

### Common Issues & Solutions

#### "Timeout exceeded" Errors
```bash
# Increase timeout and retries
uv run python -m scraping.cli https://slow-site.com \
    --timeout 60000 \
    --retries 5
```

#### Still Getting Blocked
```bash
# Use different proxy and user agent rotation
uv run python -m scraping.cli https://protected-site.com \
    --stealth \
    --proxy socks5://residential-proxy:1080 \
    --retries 3
```

#### Memory Issues with Large Sites
```bash
# Use quick mode and disable monitoring
uv run python -m scraping.cli https://large-site.com \
    --quick \
    --no-monitoring \
    --no-interactions
```

### Debug Mode
```python
# Enable debug logging
scraper = DynamicWebScraper(
    headless=False,  # Show browser
    stealth_mode=True
)

results = scraper.comprehensive_scrape(url)
print(f"Protection detected: {results.get('final_structure', {}).get('protection_detected')}")
```

## ğŸ”„ Updates & Maintenance

The bot protection landscape evolves constantly. To maintain effectiveness:

1. **Update user agents** monthly
2. **Monitor protection changes** on target sites
3. **Test stealth effectiveness** regularly
4. **Update browser versions** with Playwright
5. **Contribute improvements** back to the project

### Updating Playwright
```bash
# Update to latest browser versions
uv run playwright install chromium --force
```

## ğŸ“ Project Structure

```
scraping/
â”œâ”€â”€ src/scraping/           # Main package
â”‚   â”œâ”€â”€ __init__.py        # Package initialization
â”‚   â”œâ”€â”€ scraper.py         # Main orchestrator with stealth mode
â”‚   â”œâ”€â”€ utils.py           # Stealth utilities and helpers
â”‚   â”œâ”€â”€ structure_capture.py  # DOM analysis
â”‚   â”œâ”€â”€ dynamic_content.py    # Expandable content handling
â”‚   â”œâ”€â”€ content_monitor.py    # Real-time monitoring
â”‚   â”œâ”€â”€ spec_generator.py     # LLM specification generation
â”‚   â””â”€â”€ cli.py            # Command-line interface
â”œâ”€â”€ analyze_website.py     # Simple standalone script
â”œâ”€â”€ test_stealth.py       # Stealth mode testing
â”œâ”€â”€ test_hims_debug.py    # Debug script for protected sites
â”œâ”€â”€ BOT_PROTECTION_GUIDE.md  # Detailed bypass guide
â”œâ”€â”€ README.md             # This file
â””â”€â”€ pyproject.toml        # Dependencies
```

## ğŸ¤ Contributing

We welcome contributions! Areas where help is needed:

1. **New protection bypass techniques**
2. **Additional browser fingerprint masking**
3. **More realistic behavior simulation**
4. **Extended proxy support**
5. **Performance optimizations**

## ğŸ“ Support

1. **Check the troubleshooting section** above
2. **Review the Bot Protection Guide** (`BOT_PROTECTION_GUIDE.md`)
3. **Test with debug mode** to understand failures
4. **Search existing issues** for similar problems
5. **Create detailed bug reports** with reproduction steps

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**âš ï¸ Disclaimer**: This toolkit is designed for legitimate web scraping and research purposes. Users are responsible for complying with website terms of service, robots.txt files, and applicable laws. The bot protection bypass features should only be used for authorized testing, research, or accessing publicly available data.
